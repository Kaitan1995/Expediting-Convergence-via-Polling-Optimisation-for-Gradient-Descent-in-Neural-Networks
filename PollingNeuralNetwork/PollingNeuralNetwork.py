# -*- coding: utf-8 -*-
"""PollingNeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Izri7yNkk7SrHVCtiu0sWmEXaZHrv8bI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from warnings import filterwarnings
import os
import sys

import tkinter as tk
from   tkinter import filedialog as fd

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

#suppress warnings
filterwarnings('ignore')

def createGUI():
    btn1       = None
    inputFile  = ""
    win  = tk.Tk()
    #----------------------------------------------------------------------------------------------
    def threadQuit():
        win.destroy()
        sys.exit(0)
    #----------------------------------------------------------------------------------------------
    def threadContinue():
        win.destroy()
    #----------------------------------------------------------------------------------------------
    def btnCallBack1():
        global inputFile
        filetypes = (
                        ('csv file', '*.csv'),
                    )
        inputFile = fd.askopenfilename(
                                        title='Select .csv input file to convert',
                                        initialdir='/',
                                        filetypes=filetypes
                                     )
        if inputFile != '':
            if not btn2 is None:
                label2.config(state=tk.NORMAL)
                btn2.config(state=tk.NORMAL)
    #----------------------------------------------------------------------------------------------
    def btnCallBack2():        
        threadContinue()
    #----------------------------------------------------------------------------------------------         
    win.protocol("WM_DELETE_WINDOW", threadQuit )
    win.title('Ordinary Neural Network')
    win.resizable( 0, 0 )
    win.geometry('340x170')
    #----------------------------------------------------------------------------------------------
    label1 = tk.Label( win, text="(1) Select .csv input file: " , height=4 )
    label1.place(x=15,y=0)
    #----------------------------------------------------------------------------------------------
    label2 = tk.Label( win, text="Designed by Tan Ren Kai \n Newcastle University upon Tyne \n Agency for Science, Technology and Research \n (National Metrology Centre)" , height=4 )
    label2.place(x=50,y=100)
    #----------------------------------------------------------------------------------------------
    btn1 = tk.Button(win, text ="Select Input", command = btnCallBack1 )
    btn1.place(x=250,y=20)
    #----------------------------------------------------------------------------------------------
    btn2 = tk.Button(win, text ="Continue", command = btnCallBack2 )
    btn2.place(x=145,y=70)
    btn2.config(state=tk.DISABLED)
    #----------------------------------------------------------------------------------------------
    win.eval("tk::PlaceWindow . center")
    return inputFile, win

inputFile, win = createGUI()
win.mainloop()

raw_data = pd.read_csv(inputFile) # MNIST Database. Has 42,000 training data with pixel resolution of 784 pixels
raw_data = raw_data.sample(frac = 1) # Shuffle the 42,000 examples to be used for training/testing. Used for fairness.
raw_data = raw_data.reset_index(drop=True)
m = len(raw_data) # m is 42,000: 42,000 examples
n = len(raw_data.columns) # n is 785: 785 pixels (technically should be 784 because 1 column reserved for label)
processed_data = np.array(raw_data) # Converted into array format

processed_data_train = processed_data[1000:m].T # Transposes a random 41,000 examples. X-axis is now 785 and Y-axis is 41,000
label_array_train = processed_data_train[0] # Print(label_array_train.shape) - this is the array of the labels of the 41,000 examples. For example, is this picture 5, 6, 7, etc.?
X_Y_axis_train = processed_data_train[1:n] # Print(X_Y_axis_train.shape) - this is the array of pixels (x-axis) and examples (y-axis)
X_Y_axis_train = X_Y_axis_train / 255. # Grayscale normalisation, https://stackoverflow.com/questions/55859716/does-normalizing-images-by-dividing-by-255-leak-information-between-train-and-te

processed_data_test = processed_data[0:1000].T
label_array_test = processed_data_test[0]
X_Y_axis_test = processed_data_test[1:n]
X_Y_axis_test = X_Y_axis_test / 255. # Grayscale normalisation. Reduces Grayscale from 0-255 to 0-1.

def calculate_absolute_error(dZ2, history_toggle):
    sum_of_errors = 0
    if history_toggle == 1:
        for i in range(0, 10-1):
            for j in range(0, 41000-1):
                if (dZ2[i][j] > 0):
                    sum_of_errors = sum_of_errors + dZ2[i][j]
    return sum_of_errors

def initial_WB_randomisation():
    # Weights and biases must also have negative representation - https://towardsdatascience.com/whats-the-role-of-weights-and-bias-in-a-neural-network-4cf7e9888a0f
    W1 = np.random.random_sample(size =(10, 784)) - 0.5 # Initial randomised weights and biases activation function for every layer. Runs once. Creates a 10 x 784 matrix
    b1 = np.random.random_sample(size =(10, 1)) - 0.5 # Creates a 10 x 1 matrix
    W2 = np.random.random_sample(size =(10, 10)) - 0.5 # Creates a 10 x 10 matrix
    b2 = np.random.random_sample(size =(10, 1)) - 0.5 # Creates a 10 x 1 matrix
    return W1, b1, W2, b2

def sigmoid(x):
    return 1.0/(1.0+np.exp(-x))

def sigmoid_deriv(x):
    s = sigmoid(x)
    return s*(1-s)

def softmax(Z):
    A = np.exp(Z) / sum(np.exp(Z))
    return A

def forward_propagation(W1, b1, W2, b2, X_Y_axis_train):
    Z1 = W1.dot(X_Y_axis_train) + b1
    A1 = sigmoid(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def one_hot_representation(label_array_train):
    one_hot = np.zeros((label_array_train.size, label_array_train.max() + 1)) # Sets the correct answer as 1.
    one_hot[np.arange(label_array_train.size), label_array_train] = 1
    one_hot = one_hot.T
    return one_hot

def backward_propagation(Z1, A1, Z2, A2, W1, W2, X_Y_axis_train, label_array_train, history_toggle):
    one_hot = one_hot_representation(label_array_train)
    dZ2 = A2 - one_hot # probability minus correct answer, correct answer represented by negative number
    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2, 1)
    dZ1 = W2.T.dot(dZ2) * sigmoid_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X_Y_axis_train.T)
    db1 = 1 / m * np.sum(dZ1, 1)
    sum_of_errors = calculate_absolute_error(dZ2, history_toggle)
    return dW1, db1, dW2, db2, sum_of_errors

def backward_propagation_initial(b1_1, b2_1, b1_2, b2_2, b1_3, b2_3, Z1_1, A1_1, Z2_1, A2_1, W1_1, W2_1, Z1_2, A1_2, Z2_2, A2_2, W1_2, W2_2, Z1_3, A1_3, Z2_3, A2_3, W1_3, W2_3, X, Y):
    one_hot_Y = one_hot_representation(Y)
    dZ2_1 = A2_1 - one_hot_Y
    dZ2_2 = A2_2 - one_hot_Y
    dZ2_3 = A2_3 - one_hot_Y

    new_sum_1 = 0
    for i in range(0, 10-1):
        for j in range(0, 41000-1):
            if (dZ2_1[i][j] < 0):
                new_sum_1 = new_sum_1 + dZ2_1[i][j]
    print("new_sum_1 is", new_sum_1)

    new_sum_2 = 0
    for i in range(0, 10-1):
        for j in range(0, 41000-1):
            if (dZ2_2[i][j] < 0):
                new_sum_2 = new_sum_2 + dZ2_2[i][j]
    print("new_sum_2 is", new_sum_2)

    new_sum_3 = 0
    for i in range(0, 10-1):
        for j in range(0, 41000-1):
            if (dZ2_3[i][j] < 0):
                new_sum_3 = new_sum_3 + dZ2_3[i][j]
    print("new_sum_3 is", new_sum_3)

    if ((new_sum_1 > new_sum_2) and (new_sum_1 > new_sum_2)):
        print("Model 1 is better")
        dZ2 = dZ2_1
        A1 = A1_1
        A2 = A2_1
        b1 = b1_1
        b2 = b2_1
        Z1 = Z1_1
        W1 = W1_1
        W2 = W2_1
    elif ((new_sum_2 > new_sum_1) and (new_sum_2 > new_sum_3)):
        print("Model 2 is better")
        dZ2 = dZ2_2
        A1 = A1_2
        A2 = A2_2
        b1 = b1_2
        b2 = b2_2
        Z1 = Z1_2
        W1 = W1_2
        W2 = W2_2
    else:
        print("Model 3 is better")
        dZ2 = dZ2_3
        A1 = A1_3
        A2 = A2_3
        b1 = b1_3
        b2 = b2_3
        Z1 = Z1_3
        W1 = W1_3
        W2 = W2_3

    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2, 1)
    dZ1 = W2.T.dot(dZ2) * sigmoid_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X.T)
    db1 = 1 / m * np.sum(dZ1, 1)
    return b1, b2, dW1, db1, dW2, db2, W1, W2, A2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
    W1 = W1 - alpha * dW1
    b1 = b1 - alpha * np.reshape(db1, (10,1))
    W2 = W2 - alpha * dW2
    b2 = b2 - alpha * np.reshape(db2, (10,1))
    return W1, b1, W2, b2

def get_predictions(A2):
    return np.argmax(A2, 0) # sets most likely answer to 1, and others to 0.

def get_accuracy(predictions, Y):
    # print(predictions, Y)
    return np.sum(predictions == Y) / Y.size

def make_predictions(X, W1, b1, W2, b2):
    _, _, _, A2 = forward_propagation(W1, b1, W2, b2, X)
    predictions = get_predictions(A2)
    return predictions

def test_prediction(index, W1, b1, W2, b2):
    current_image = X_Y_axis_test[:, index, None]
    prediction = make_predictions(X_Y_axis_test[:, index, None], W1, b1, W2, b2)
    label = label_array_test[index]
    print("Prediction: ", prediction)
    print("Label: ", label)

    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

def main_script(alpha1, alpha2, alpha3, epochs, history_toggle):
    history = []
    sum_of_errors = 0
    # W1_1, b1_1, W2_1, b2_1 = initial_WB_randomisation()
    # W1_2, b1_2, W2_2, b2_2 = initial_WB_randomisation()
    # W1_3, b1_3, W2_3, b2_3 = initial_WB_randomisation()
    # Z1_1, A1_1, Z2_1, A2_1 = forward_propagation(W1_1, b1_1, W2_1, b2_1, X_Y_axis_train)
    # Z1_2, A1_2, Z2_2, A2_2 = forward_propagation(W1_2, b1_2, W2_2, b2_2, X_Y_axis_train)
    # Z1_3, A1_3, Z2_3, A2_3 = forward_propagation(W1_3, b1_3, W2_3, b2_3, X_Y_axis_train)
    # b1, b2, dW1, db1, dW2, db2, W1, W2, A2  = backward_propagation_initial(b1_1, b2_1, b1_2, b2_2, b1_3, b2_3, Z1_1, A1_1, Z2_1, A2_1, W1_1, W2_1, Z1_2, A1_2, Z2_2, A2_2, W1_2, W2_2, Z1_3, A1_3, Z2_3, A2_3, W1_3, W2_3, X_Y_axis_train, label_array_train)

    W1, b1, W2, b2 = initial_WB_randomisation()
    Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X_Y_axis_train)
    dW1, db1, dW2, db2, sum_of_errors = backward_propagation(Z1, A1, Z2, A2, W1, W2, X_Y_axis_train, label_array_train, history_toggle)

    for i in range(epochs):

        # Randomised testing
        W1_1, b1_1, W2_1, b2_1 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha1)
        W1_2, b1_2, W2_2, b2_2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha2)
        W1_3, b1_3, W2_3, b2_3 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha3)

        # array = W1_1 - W1_2
        # result = list(map(sum, array))
        # print("Total difference in Weights", sum(result))


        Z1_1, A1_1, Z2_1, A2_1 = forward_propagation(W1_1, b1_1, W2_1, b2_1, X_Y_axis_train)
        Z1_2, A1_2, Z2_2, A2_2 = forward_propagation(W1_2, b1_2, W2_2, b2_2, X_Y_axis_train)
        Z1_3, A1_3, Z2_3, A2_3 = forward_propagation(W1_3, b1_3, W2_3, b2_3, X_Y_axis_train)
        if history_toggle == 1:
            history.append(sum_of_errors)
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions_1 = get_predictions(A2_1)
            prediction_choice_1 = get_accuracy(predictions_1, label_array_train)
            print(prediction_choice_1)
            predictions_2 = get_predictions(A2_2)
            prediction_choice_2 = get_accuracy(predictions_2, label_array_train)
            print(prediction_choice_2)
            predictions_3 = get_predictions(A2_3)
            prediction_choice_3 = get_accuracy(predictions_3, label_array_train)
            print(prediction_choice_3)

        if ((prediction_choice_1 > prediction_choice_2) and (prediction_choice_1 > prediction_choice_3)):
            print("Prediction_1 is better:", prediction_choice_1)
            W1 = W1_1
            b1 = b1_1
            W2 = W2_1
            b2 = b2_1
            Z1 = Z1_1
            A1 = A1_1
            Z2 = Z2_1
            A2 = A2_1
        elif ((prediction_choice_2 > prediction_choice_1) and (prediction_choice_2 > prediction_choice_3)):
            print("Prediction_2 is better:", prediction_choice_2)
            W1 = W1_2
            b1 = b1_2
            W2 = W2_2
            b2 = b2_2
            Z1 = Z1_2
            A1 = A1_2
            Z2 = Z2_2
            A2 = A2_2
        elif ((prediction_choice_3 > prediction_choice_1) and (prediction_choice_3 > prediction_choice_2)):
            print("Prediction_3 is better:", prediction_choice_3)
            W1 = W1_3
            b1 = b1_3
            W2 = W2_3
            b2 = b2_3
            Z1 = Z1_3
            A1 = A1_3
            Z2 = Z2_3
            A2 = A2_3
        else:
            print("Special:", prediction_choice_3)
            W1 = W1_3
            b1 = b1_3
            W2 = W2_3
            b2 = b2_3
            Z1 = Z1_3
            A1 = A1_3
            Z2 = Z2_3
            A2 = A2_3

        dW1, db1, dW2, db2, sum_of_errors = backward_propagation(Z1, A1, Z2, A2, W1, W2, X_Y_axis_train, label_array_train, history_toggle)

    for i in range (3):
        test_prediction(i, W1, b1, W2, b2)

    if history_toggle == 1:
        plt.plot(history)
        plt.title('Polling Neural Network, Epochs:100')
        plt.ylabel('Absolute Difference Error')
        plt.xlabel('Training Iteration')
        plt.show()

if __name__ == "__main__":
    history_toggle = 1
    alpha1 = 0.1
    alpha2 = 0.5
    alpha3 = 0.9
    epochs = 100
    main_script(alpha1, alpha2, alpha3, epochs, history_toggle)